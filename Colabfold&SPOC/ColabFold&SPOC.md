## Important Links

The repo for ColabFold in your local PC is here: https://github.com/YoshitakaMo/localcolabfold

The tutorial for running ColabFold notebook on a HPC over SSH is here: https://www.jameslingford.com/blog/colabfold-hpc-ssh-howto/

The repo for SPOC is here: https://github.com/walterlab-HMS/SPOC

# Run ColabFold

## What is ColabFold and how is it compared to alphafold/2.3.1
ColabFold
- Optimized version of AlphaFold (developed by Mirdita et al.)
- Replaces JackHMMER with MMseqs2 (ultra-fast multiple sequence alignments, MSA).
- Optional smaller databases (e.g., ColabFoldDB) — fits on a laptop/server (~100 GB).
- 5–10x faster inference time.
- Slight tweaks to internal parameters for better multimer handling.
- Slightly lower MSA depth compared to full AlphaFold, but not noticeable for most proteins.
- Supports batch prediction, faster amber relaxation, etc.

AlphaFold 2.3.1
- Original DeepMind release.
- JackHMMER search → very slow but very deep MSAs.
- Requires huge databases like BFD, Uniref90, MGnify (over 2 TB total).
- Very accurate especially for orphan proteins (no homologs).
- Running time: hours per complex if the sequence is long.

## What is the difference between local ColabFold and ColabFold?
ColabFold is a cloud-based version of the AlphaFold model that runs on Google Colab. It provides a convenient way to use AlphaFold without needing to set up the environment locally.

Local ColabFold is essentially the same software as ColabFold, but it's installed and run locally on your system or an HPC cluster. You set it up manually, often in a Python environment with dependencies. This is the one for HPC.

### Create .sh file: ``` nano protein1_protein2.sh ```

### Copy and paste the following script in

This code will submit the the same number of jobs to gpu as to the number of predictions you want to make. You can find the colabfold ouput in the same folder as the fasta sequence file. this step incorpate the generation of  
    
- Step 1: fetch all the fasta sequences for each multimers, For multiple multimer FASTA files provided in the .csv file. In the following code, I am excluding the protein paris whose amino acid length > 3600. If you want to generate fasta file regardless, just delete that block.
 See Get colabfold fasta file. ipynb
    - Running 3 models because that's what spoc were training on.

# SPOC Set-up
## What is SPOC and why we use it?

### Clone the SPOC repository
- To clone the SPOC repository, use the following command:
  ``` bash
  git clone https://github.com/walterlab-HMS/SPOC.git
  ```
- Navigate into the cloned directory: ```cd SPOC```

### Create environment to load necessary dependencies

If you are using conda or miniconda, please refer to original repo. The following is for micromamba

- create the environment:
  ```bash
  micromamba env create -f SPOC/environment.yml
  ```
- activate the environment:
  ``` bash
  micromamba activate spoc_venv
  ```

### Run SPOC 
Here is an example input folder:
```bash
my_afm_predictions_folder/
│-- DONS_HUMAN__MCM3_HUMAN__1374aa.a3m.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_scores_rank_001_alphafold2_multimer_v3_model_1_seed_000.json.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_scores_rank_002_alphafold2_multimer_v3_model_2_seed_000.json.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_scores_rank_003_alphafold2_multimer_v3_model_3_seed_000.json.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_unrelaxed_rank_001_alphafold2_multimer_v3_model_1_seed_000.pdb.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_unrelaxed_rank_002_alphafold2_multimer_v3_model_2_seed_000.pdb.xz
│-- DONS_HUMAN__MCM3_HUMAN__1374aa_unrelaxed_rank_003_alphafold2_multimer_v3_model_3_seed_000.pdb.xz

```
- contains a file with .a3m (which is same for all the model)
- three .json files and three .pdb files generated by colabfold
- Run the prediction by
```bash
python3 run.py my_afm_predictions_folder
```

---
# Run Batch files for colabfold and SPOC
## Colabfold
### Example Script for a low number of tasks at once (<20):
```bash
    #!/bin/bash
    # Go to the folder that contains all the folders for fasta file
    cd All_multimer
    
    # Loop through each folder containing the FASTA file
    for dir in */; do
        # Remove trailing slash to get folder name
        folder=${dir%/}
    
        # Find the fasta file inside the folder
        fasta=$(find "$folder" -maxdepth 1 -name "*.fasta" | head -n 1)
    
        # Skip if no fasta file found
        if [[ -z "$fasta" ]]; then
            echo "No FASTA in $folder, skipping..."
            continue
        fi
    
        # Set job and output file base names based on folder
        base_name=$(basename "$fasta" .fasta)
    
        # Generate a temporary job script
        job_script="${folder}/run_colabfold.bsub"
    
        cat > "$job_script" <<EOF
    #!/bin/bash
    #BSUB -q gpu
    #BSUB -R "rusage[mem=20G]"
    #BSUB -J ${base_name}_models
    #BSUB -gpu "num=1"
    #BSUB -n 1
    #BSUB -W 2:00
    #BSUB -oo ${folder}/${base_name}.out
    #BSUB -eo ${folder}/${base_name}.err
    
    module load localcolabfold/1.5.5
    LOCALCOLABIMG=/share/pkg/containers/localcolabfold/localcolabfold-1.5.5.sif
    
    singularity exec --nv $LOCALCOLABIMG colabfold_batch \
         --templates --num-recycle 3 --num-ensemble 1 --num-models 3 "$fasta" "${folder}"
    
    EOF
    
        # Submit the job
        bsub < "$job_script"
    done

```
### Example Script for a large number of tasks, e.g., ~2000
- Step 1: Make sure you have the run_wrapper.py in the same directory as the run.py

  Create a new sh file within the SPOC folder: ```nano run_wrapper.py```

  Copy and paste the following script into the file:

  This code will create a temporary directory to only take the a3m, .json, and .pdb files from the colabfold output and runs run.py on thie temporary directory
    See script in the same folder
- Step 2: Create a file at the same directory as the input_folders.txt:```nano run_prediction.sh```
    See script in the same folder
